{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a7f43c8-7809-4ae2-9a65-6fead8554c1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## LLave Primaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28513f35-965c-43f5-849a-8cc772fefb68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "\n",
    "#client = Socrata(\"www.datos.gov.co\", None)\n",
    "\n",
    "token = dbutils.secrets.get(\"claves\",\"token_app\")\n",
    "codigo_dataset = dbutils.widgets.get(\"codigo_dataset\").strip()\n",
    "\n",
    "# ‚úÖ Aqu√≠ el cambio m√≠nimo y m√°gico\n",
    "client = Socrata(\"www.datos.gov.co\", str(token),timeout=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ecfb9a4-4520-4ef9-bb01-10417c004285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"codigo_dataset\", \"\")\n",
    "print(\"üß™ Widget valor:\", repr(dbutils.widgets.get(\"codigo_dataset\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b072024-379c-4c21-984c-26fb50c2e71c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = Socrata(\"www.datos.gov.co\", \"KwZhcbkekhw5FpAUWVnzvIdVA\", None)\n",
    "client.get(\"rpmr-utcd\", query=\"SELECT numero_de_proceso LIMIT 1 OFFSET 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d961ff-ad51-4b2f-b0aa-be22b1ba7888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# 3. Par√°metros de paginaci√≥n\n",
    "limit = 500000\n",
    "offset = 0\n",
    "write_mode = \"overwrite\"\n",
    "reintentos = 5\n",
    "\n",
    "print(f\"Iniciando carga por lotes para el dataset: {codigo_dataset}\")\n",
    "\n",
    "# 4. Bucle para obtener y cargar los datos por lotes\n",
    "while True:\n",
    "    intentos = 0\n",
    "\n",
    "    while intentos < reintentos:\n",
    "        try:\n",
    "            print(f\"Obteniendo y cargando lote {offset}...\")\n",
    "            # Construye y ejecuta la consulta para el lote actual\n",
    "            query = f\"SELECT numero_del_contrato, numero_de_proceso, nit_de_la_entidad, documento_proveedor, estado_del_proceso LIMIT {limit} OFFSET {offset}\"\n",
    "            results = client.get(codigo_dataset, query=query) \n",
    "\n",
    "            # Si la API no devuelve m√°s registros, se termina el script\n",
    "            if not results:\n",
    "                print(\"‚úÖ Carga de datos finalizada. No hay m√°s registros.\")\n",
    "                exit(0)\n",
    "\n",
    "            # Convierte el lote a un DataFrame de Spark y lo escribe en la tabla Delta\n",
    "            spark.createDataFrame(results).write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(write_mode) \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(\"main.diplomado_datos.ids_contratos_procesos\")\n",
    "\n",
    "            print(f\"Lote de {len(results)} registros desde offset {offset} cargado.\")\n",
    "\n",
    "            # Se cambia a modo 'append' para las siguientes iteraciones y se incrementa el offset\n",
    "            write_mode = \"append\"\n",
    "            offset += limit\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            intentos += 1\n",
    "            print(\"Error al obtener o cargar el lote:\", e)\n",
    "            print(f\"Intento {intentos} de {reintentos}...\")\n",
    "            time.sleep(20)\n",
    "    else:\n",
    "        print(f\"‚ùå No se pudo obtener datos en offset {offset} despu√©s de {reintentos} intentos. Asumimos que no hay m√°s registros.\")\n",
    "        print(\"‚úÖ Carga de datos finalizada con √©xito.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "696c0653-5305-4b9d-aa0f-de5babb32378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_secop_id = spark.table(\"main.diplomado_datos.ids_contratos_procesos\")\n",
    "df_secop_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a0174b2-b0ec-4233-b802-78368cebc089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar las funciones necesarias de PySpark\n",
    "from pyspark.sql.functions import sha2, concat_ws, col\n",
    "\n",
    "# 1. Cargar la tabla correcta desde el cat√°logo a un DataFrame\n",
    "# Se asume que esta es la tabla que contiene las columnas que mencionaste.\n",
    "df_secop_id = spark.table(\"main.diplomado_datos.ids_contratos_procesos\")\n",
    "\n",
    "# 2. Definir la lista corregida de columnas para el identificador √∫nico\n",
    "columnas_para_hash = [\n",
    "    \"numero_del_contrato\",\n",
    "    \"numero_de_proceso\",\n",
    "    \"nit_de_la_entidad\",\n",
    "    \"documento_proveedor\",\n",
    "    \"estado_del_proceso\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7877061-422c-4ef7-bf22-5eb380028717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. A√±adir la nueva columna 'id_unico'\n",
    "# Se concatenan las columnas clave con un separador y se les aplica un hash SHA-2.\n",
    "df_con_id = df_secop_id.withColumn(\n",
    "    \"id_unico_con_estado\",\n",
    "    sha2(concat_ws(\"||\", *[col(c) for c in columnas_para_hash]), 256)\n",
    ")\n",
    "\n",
    "columnas_para_hash_se = [\n",
    "    \"numero_del_contrato\",\n",
    "    \"numero_de_proceso\",\n",
    "    \"nit_de_la_entidad\",\n",
    "    \"documento_proveedor\"\n",
    "]\n",
    "df_con_id = df_con_id.withColumn(\n",
    "    \"id_unico_sin_estado\",\n",
    "    sha2(concat_ws(\"||\", *[col(c) for c in columnas_para_hash_se]), 256)\n",
    ")\n",
    "df_con_id.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d882894b-5d67-430b-b473-370059b20f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_con_id.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"main.diplomado_datos.secop_id_bronze\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "sodapy==2.2.0"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Descarga.ID.ipynb",
   "widgets": {
    "codigo_dataset": {
     "currentValue": "rpmr-utcd",
     "nuid": "79f0c165-bf74-4b81-9922-395893294ff7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "codigo_dataset",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "codigo_dataset",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
